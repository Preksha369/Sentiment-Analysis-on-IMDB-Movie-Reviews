{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Sentiment Analysis on IMDB Movie Reviews\n",
        "##Introduction\n",
        "This notebook demonstrates a complete workflow for performing sentiment analysis on the popular IMDB movie reviews dataset.\n",
        "The project covers loading and preprocessing the data, extracting meaningful features using Count Vectorizer and TF-IDF techniques, and training multiple machine learning classifiers including Naive Bayes, Logistic Regression, Decision Trees, Random Forests, and XGBoost.\n",
        "\n",
        "By evaluating model performance through accuracy scores, this notebook helps identify the most effective methods for classifying movie reviews as positive or negative.\n",
        "The modular design makes it easy to extend or adapt for similar text classification tasks."
      ],
      "metadata": {
        "id": "EG16F4wNGjxF"
      },
      "id": "EG16F4wNGjxF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "At first, we import all the essential Python libraries and modules needed for the sentiment analysis project.  \n",
        "These include libraries for data manipulation (`pandas`, `numpy`), data preprocessing, feature extraction, machine learning models, and evaluation metrics.\n"
      ],
      "metadata": {
        "id": "WPnsAH9kDL7p"
      },
      "id": "WPnsAH9kDL7p"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3562ca1e-f88b-4f36-89e8-c3f3fb85416c",
      "metadata": {
        "id": "3562ca1e-f88b-4f36-89e8-c3f3fb85416c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Now we load the IMDB movie reviews dataset from a CSV file.  \n",
        "We use the 'python' engine in pandas to handle potential parsing issues gracefully.  \n",
        "Additionally, we extract the first 1000 rows to create a smaller subset for quicker experimentation and save it as a new CSV file.\n"
      ],
      "metadata": {
        "id": "wi-kWGByDipb"
      },
      "id": "wi-kWGByDipb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "913d7d1d-e0b4-4d9d-8902-b76c43c1e6d2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "913d7d1d-e0b4-4d9d-8902-b76c43c1e6d2",
        "outputId": "5bd48d60-78f5-4e5e-ce49-8208ce0f6270"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 1000 rows saved as 'IMDB_Dataset_1000.csv'\n"
          ]
        }
      ],
      "source": [
        "# Load the original dataset\n",
        "# Using the 'python' engine for better handling of potential parsing issues\n",
        "try:\n",
        "    df = pd.read_csv(\"IMDB_Dataset.csv\", engine='python')\n",
        "except ParserError as e:\n",
        "    print(f\"Error reading IMDB_Dataset.csv: {e}\")\n",
        "    print(\"Attempting to read with different parameters or inspect the file.\")\n",
        "\n",
        "    pass\n",
        "\n",
        "\n",
        "if 'df' in locals():\n",
        "    # Extract the first 1000 rows\n",
        "    df_top_1000 = df.head(1000)\n",
        "\n",
        "    # Save it as a new CSV file\n",
        "    df_top_1000.to_csv(\"IMDB_Dataset_1000.csv\", index=False)\n",
        "\n",
        "    print(\"Top 1000 rows saved as 'IMDB_Dataset_1000.csv'\")\n",
        "else:\n",
        "    print(\"Failed to load IMDB_Dataset.csv. Cannot proceed with creating IMDB_Dataset_1000.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "The function, `load_data`, is designed to load the preprocessed subset of the IMDB dataset from a CSV file (`IMDB_Dataset_1000.csv`).  \n",
        "\n",
        "It selects only the relevant columns — `review` and `sentiment` — which are required for the sentiment analysis task, and returns the filtered dataframe.\n"
      ],
      "metadata": {
        "id": "p446cqSYEAdK"
      },
      "id": "p446cqSYEAdK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5b86417-7c4e-4924-be31-f8eac019dfb0",
      "metadata": {
        "id": "d5b86417-7c4e-4924-be31-f8eac019dfb0"
      },
      "outputs": [],
      "source": [
        "# Function to load dataset\n",
        "def load_data(filepath):\n",
        "    print(\"Dataset Loaded Successfully\")\n",
        "    df = pd.read_csv(\"IMDB_Dataset_1000.csv\")\n",
        "    df = df[['review', 'sentiment']]  # Ensure only relevant columns\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "The function `preprocess_data` takes the loaded dataframe as input and prepares it for model training by:  \n",
        "- Separating the features (`review` texts) and labels (`sentiment`).  \n",
        "- Encoding the sentiment labels (`positive` and `negative`) into numerical format (1 and 0) using Label Encoding.  \n",
        "- Splitting the data into training and testing sets with an 80-20 ratio to evaluate model performance on unseen data.\n"
      ],
      "metadata": {
        "id": "cKzT1gu6EKTt"
      },
      "id": "cKzT1gu6EKTt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e9bf652-642d-4e3a-8e00-62352ab84a32",
      "metadata": {
        "id": "3e9bf652-642d-4e3a-8e00-62352ab84a32"
      },
      "outputs": [],
      "source": [
        "# Function to preprocess data and split\n",
        "def preprocess_data(df):\n",
        "    X = df['review']\n",
        "    y = df['sentiment']\n",
        "\n",
        "    # Apply Label Encoding to convert 'positive'/'negative' → 1/0\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "    return train_test_split(X, y_encoded, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "The function `extract_features` converts raw text data into numerical features that machine learning models can understand.  \n",
        "\n",
        "It supports two vectorization methods:  \n",
        "- **Count Vectorization**: Counts the frequency of words (excluding English stopwords).  \n",
        "- **TF-IDF Vectorization**: Weighs words by their importance using Term Frequency-Inverse Document Frequency, also removing stopwords.  \n",
        "\n",
        "The function fits the vectorizer on the training data and transforms both training and test sets accordingly.\n"
      ],
      "metadata": {
        "id": "SZjy2U8oFLPQ"
      },
      "id": "SZjy2U8oFLPQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfb563e3-4dd5-4285-a993-fc41589c21f6",
      "metadata": {
        "id": "dfb563e3-4dd5-4285-a993-fc41589c21f6"
      },
      "outputs": [],
      "source": [
        "# Function to extract features (CountVectorizer / TF-IDF) with stopword removal\n",
        "def extract_features(X_train, X_test, method=\"count\"):\n",
        "    if method == \"count\":\n",
        "        vectorizer = CountVectorizer(stop_words='english')  # Removing stopwords\n",
        "    elif method == \"tfidf\":\n",
        "        vectorizer = TfidfVectorizer(stop_words='english')  # Removing stopwords\n",
        "    else:\n",
        "        raise ValueError(\"Method should be 'count' or 'tfidf'\")\n",
        "\n",
        "    X_train_transformed = vectorizer.fit_transform(X_train)\n",
        "    X_test_transformed = vectorizer.transform(X_test)\n",
        "\n",
        "    return X_train_transformed, X_test_transformed"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "This function `train_and_evaluate` trains a specified machine learning model on the training data and evaluates its accuracy on the test data.  \n",
        "\n",
        "Supported models include:  \n",
        "- Naive Bayes  \n",
        "- Logistic Regression  \n",
        "- Decision Tree  \n",
        "- Random Forest  \n",
        "- XGBoost  \n",
        "\n",
        "The function returns the accuracy score as a performance metric.\n"
      ],
      "metadata": {
        "id": "2y9Yv4E_FwM-"
      },
      "id": "2y9Yv4E_FwM-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8f10d0c-2048-44fa-8b66-f2a1db205ca6",
      "metadata": {
        "id": "e8f10d0c-2048-44fa-8b66-f2a1db205ca6"
      },
      "outputs": [],
      "source": [
        "# Function to train and evaluate models\n",
        "def train_and_evaluate(X_train, X_test, y_train, y_test, model_name):\n",
        "    models = {\n",
        "        \"naive_bayes\": MultinomialNB(),\n",
        "        \"logistic_regression\": LogisticRegression(max_iter=1000),\n",
        "        \"decision_tree\": DecisionTreeClassifier(),\n",
        "        \"random_forest\": RandomForestClassifier(n_estimators=100),\n",
        "        \"xgboost\": XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
        "    }\n",
        "\n",
        "    model = models[model_name]\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    return accuracy_score(y_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "This function `sentiment_analysis_pipeline` orchestrates the entire workflow from data loading to model evaluation:  \n",
        "- Loads the dataset from a CSV file.  \n",
        "- Preprocesses the data (including label encoding and train-test splitting).  \n",
        "- Extracts features using both Count Vectorizer and TF-IDF methods.  \n",
        "- Trains and evaluates five different classifiers on each feature set.  \n",
        "\n",
        "The results (accuracy scores) of each model-feature combination are collected and returned as a pandas DataFrame for easy comparison.\n"
      ],
      "metadata": {
        "id": "loWsU3oYF6hh"
      },
      "id": "loWsU3oYF6hh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "059c0048-62fc-45ae-8318-766a2cd00890",
      "metadata": {
        "id": "059c0048-62fc-45ae-8318-766a2cd00890",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9069bc6f-26ac-440e-a12f-a155e3779d5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Loaded Successfully\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:29:37] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [17:29:40] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                           Accuracy\n",
            "count_naive_bayes             0.775\n",
            "count_logistic_regression     0.790\n",
            "count_decision_tree           0.705\n",
            "count_random_forest           0.770\n",
            "count_xgboost                 0.775\n",
            "tfidf_naive_bayes             0.835\n",
            "tfidf_logistic_regression     0.785\n",
            "tfidf_decision_tree           0.660\n",
            "tfidf_random_forest           0.790\n",
            "tfidf_xgboost                 0.730\n"
          ]
        }
      ],
      "source": [
        "# Pipeline function to run the entire analysis\n",
        "def sentiment_analysis_pipeline(filepath):\n",
        "    df = load_data(filepath)\n",
        "    X_train, X_test, y_train, y_test = preprocess_data(df)  # Label encoding applied here\n",
        "\n",
        "    methods = [\"count\", \"tfidf\"]\n",
        "    models = [\"naive_bayes\", \"logistic_regression\", \"decision_tree\", \"random_forest\", \"xgboost\"]\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for method in methods:\n",
        "        X_train_transformed, X_test_transformed = extract_features(X_train, X_test, method)\n",
        "\n",
        "        for model in models:\n",
        "            key = f\"{method}_{model}\"\n",
        "            results[key] = train_and_evaluate(X_train_transformed, X_test_transformed, y_train, y_test, model)\n",
        "\n",
        "    return pd.DataFrame(results, index=[\"Accuracy\"]).T\n",
        "\n",
        "# Run the full pipeline\n",
        "results_df = sentiment_analysis_pipeline(\"TMDB_Dataset_1000.csv\")\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results Interpretation\n",
        "\n",
        "- The table shows accuracy scores for different classifiers combined with Count Vectorizer and TF-IDF feature extraction methods.  \n",
        "- TF-IDF with Naive Bayes achieved the highest accuracy of 83.5%, indicating strong performance with weighted term importance.  \n",
        "- Logistic Regression and Random Forest also performed well across both feature extraction techniques.  \n",
        "- Decision Tree showed relatively lower accuracy, suggesting it may not be the best choice for this dataset and task.  \n",
        "- These insights guide us toward selecting the most effective model and feature extraction strategy for sentiment analysis on the IMDB dataset.\n"
      ],
      "metadata": {
        "id": "PsGrOriNGDuQ"
      },
      "id": "PsGrOriNGDuQ"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}